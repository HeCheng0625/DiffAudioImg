{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=[\"/home/v-yuancwang/DiffAudioImg/metadata/vgg_train_0.json\",\n",
    "                                           \"/home/v-yuancwang/DiffAudioImg/metadata/vgg_train_1.json\",\n",
    "                                           \"/home/v-yuancwang/DiffAudioImg/metadata/vgg_train_2.json\",\n",
    "                                           \"/home/v-yuancwang/DiffAudioImg/metadata/vgg_train_3.json\",\n",
    "                                           \"/home/v-yuancwang/DiffAudioImg/metadata/vgg_train_4.json\"])\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ControlNet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scheduler and models\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_unet(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = vae.encode(dataset[0][\"pixel_values\"].reshape(1,3,256,256)).latent_dist.sample()\n",
    "latents = latents * vae.config.scaling_factor\n",
    "print(latents.shape)\n",
    "\n",
    "# Sample noise that we'll add to the latents\n",
    "noise = torch.randn_like(latents)\n",
    "bsz = latents.shape[0]\n",
    "# Sample a random timestep for each image\n",
    "timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "timesteps = timesteps.long()\n",
    "\n",
    "# Add noise to the latents according to the noise magnitude at each timestep\n",
    "# (this is the forward diffusion process)\n",
    "noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "# Get the text embedding for conditioning\n",
    "encoder_hidden_states = torch.zeros((1, 77, 768))\n",
    "\n",
    "controlnet_image = dataset[0][\"conditioning_pixel_values\"].reshape(1,3,256,256)\n",
    "\n",
    "down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "    noisy_latents,\n",
    "    timesteps,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    controlnet_cond=controlnet_image,\n",
    "    return_dict=False,\n",
    "    )\n",
    "\n",
    "# Predict the noise residual\n",
    "model_pred = unet(\n",
    "    noisy_latents,\n",
    "    timesteps,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    down_block_additional_residuals=down_block_res_samples,\n",
    "    mid_block_additional_residual=mid_block_res_sample,\n",
    "    ).sample\n",
    "\n",
    "print(model_pred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# Let's load the popular vermeer image\n",
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\")\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler, PNDMScheduler\n",
    "\n",
    "pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# this command loads the individual model components on GPU on-demand.\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.manual_seed(1342)\n",
    "\n",
    "out_image = pipe(\n",
    "    \"Dragon magic girl\", num_inference_steps=100, generator=generator, image=canny_image, guidance_scale=8, width=512, height=512\n",
    ").images[0]\n",
    "\n",
    "out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/blob/v-yuancwang/DiffAudioImg/VGGSound/data/vggsound/img_spilt\"\n",
    "img = Image.open(os.path.join(img_path, \"2/--UQXuZfbFY_44000_54000_2.jpg\"))\n",
    "img = img.resize((512, 512))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.resize((128, 128))\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
