{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/v-yuancwang/.cache/huggingface/datasets/json/default-2b99dd54ed7286d7/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(__name__)\n",
    "\n",
    "def make_train_dataset():\n",
    "\n",
    "    dataset = Dataset.from_json(\"/home/v-yuancwang/DiffAudioImg/metadata/vgg_train_2.json\")\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize inputs and targets.\n",
    "    column_names = dataset.column_names\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    # if args.image_column is None:\n",
    "    image_column = column_names[0]\n",
    "    #     logger.info(f\"image column defaulting to {image_column}\")\n",
    "    # else:\n",
    "    #     image_column = args.image_column\n",
    "    #     if image_column not in column_names:\n",
    "    #         raise ValueError(\n",
    "    #             f\"`--image_column` value '{args.image_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}\"\n",
    "    #         )\n",
    "\n",
    "    # if args.caption_column is None:\n",
    "    caption_column = column_names[2]\n",
    "    #     logger.info(f\"caption column defaulting to {caption_column}\")\n",
    "    # else:\n",
    "    #     caption_column = args.caption_column\n",
    "    #     if caption_column not in column_names:\n",
    "    #         raise ValueError(\n",
    "    #             f\"`--caption_column` value '{args.caption_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}\"\n",
    "    #         )\n",
    "\n",
    "    # if args.conditioning_image_column is None:\n",
    "    conditioning_image_column = column_names[1]\n",
    "    #     logger.info(f\"conditioning image column defaulting to {caption_column}\")\n",
    "    # else:\n",
    "    #     conditioning_image_column = args.conditioning_image_column\n",
    "    #     if conditioning_image_column not in column_names:\n",
    "    #         raise ValueError(\n",
    "    #             f\"`--conditioning_image_column` value '{args.conditioning_image_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}\"\n",
    "    #         )\n",
    "\n",
    "    # def tokenize_captions(examples, is_train=True):\n",
    "    #     captions = []\n",
    "    #     for caption in examples[caption_column]:\n",
    "    #         if random.random() < 0.5:\n",
    "    #             captions.append(\"\")\n",
    "    #         elif isinstance(caption, str):\n",
    "    #             captions.append(caption)\n",
    "    #         elif isinstance(caption, (list, np.ndarray)):\n",
    "    #             # take a random caption if there are multiple\n",
    "    #             captions.append(random.choice(caption) if is_train else caption[0])\n",
    "    #         else:\n",
    "    #             raise ValueError(\n",
    "    #                 f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "    #             )\n",
    "    #     inputs = tokenizer(\n",
    "    #         captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    #     )\n",
    "    #     return inputs.input_ids\n",
    "\n",
    "    image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((256,256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    conditioning_image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((256,256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def preprocess_train(examples):\n",
    "        images = [Image.open(os.path.join(\"/blob/v-yuancwang/DiffAudioImg/VGGSound/data/vggsound/img_spilt\", image)).convert(\"RGB\") \n",
    "                  for image in examples[image_column]]\n",
    "        images = [image_transforms(image) for image in images]\n",
    "\n",
    "        conditioning_images = [np.load(os.path.join(\"/blob/v-yuancwang/DiffAudioImg/VGGSound/data/vggsound/mel\", image))\n",
    "                                for image in examples[conditioning_image_column]]\n",
    "        conditioning_images = [conditioning_image_transforms(image).repeat_interleave(3, dim=0) for image in conditioning_images]\n",
    "\n",
    "        examples[\"pixel_values\"] = images\n",
    "        examples[\"conditioning_pixel_values\"] = conditioning_images\n",
    "        # examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "\n",
    "        return examples\n",
    "\n",
    "    train_dataset = dataset.with_transform(preprocess_train)\n",
    "\n",
    "    return train_dataset\n",
    "\n",
    "dataset = make_train_dataset()\n",
    "print(dataset[0][\"pixel_values\"].shape)\n",
    "print(dataset[0][\"conditioning_pixel_values\"].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ControlNet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scheduler and models\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_unet(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 32, 32])\n",
      "torch.Size([1, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "latents = vae.encode(dataset[0][\"pixel_values\"].reshape(1,3,256,256)).latent_dist.sample()\n",
    "latents = latents * vae.config.scaling_factor\n",
    "print(latents.shape)\n",
    "\n",
    "# Sample noise that we'll add to the latents\n",
    "noise = torch.randn_like(latents)\n",
    "bsz = latents.shape[0]\n",
    "# Sample a random timestep for each image\n",
    "timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "timesteps = timesteps.long()\n",
    "\n",
    "# Add noise to the latents according to the noise magnitude at each timestep\n",
    "# (this is the forward diffusion process)\n",
    "noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "# Get the text embedding for conditioning\n",
    "encoder_hidden_states = torch.zeros((1, 77, 768))\n",
    "\n",
    "controlnet_image = dataset[0][\"conditioning_pixel_values\"].reshape(1,3,256,256)\n",
    "\n",
    "down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "    noisy_latents,\n",
    "    timesteps,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    controlnet_cond=controlnet_image,\n",
    "    return_dict=False,\n",
    "    )\n",
    "\n",
    "# Predict the noise residual\n",
    "model_pred = unet(\n",
    "    noisy_latents,\n",
    "    timesteps,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    down_block_additional_residuals=down_block_res_samples,\n",
    "    mid_block_additional_residual=mid_block_res_sample,\n",
    "    ).sample\n",
    "\n",
    "print(model_pred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
