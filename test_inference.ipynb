{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from datetime import timedelta\n",
    "from accelerate.utils import ProjectConfiguration, set_seed, InitProcessGroupKwargs\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PretrainedConfig, CLIPTextModel, CLIPTokenizer, CLIPImageProcessor\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    PNDMScheduler\n",
    ")\n",
    "\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(tokenizer, text_encoder, prompt, device):\n",
    "    text_inputs = tokenizer(prompt, padding=\"max_length\", \n",
    "                            max_length=tokenizer.model_max_length, truncation=True, \n",
    "                            return_tensors=\"pt\",\n",
    "                            )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "\n",
    "    if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "        attention_mask = text_inputs.attention_mask.to(device)\n",
    "    else:\n",
    "        attention_mask = None\n",
    "\n",
    "    prompt_embeds = text_encoder(\n",
    "        text_input_ids.to(device),\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    prompt_embeds = prompt_embeds[0]\n",
    "    prompt_embeds = prompt_embeds.to(dtype=text_encoder.dtype, device=device)\n",
    "\n",
    "    uncond_tokens = [\"\"]\n",
    "    max_length = prompt_embeds.shape[1]\n",
    "    uncond_input = tokenizer(\n",
    "        uncond_tokens,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "        attention_mask = uncond_input.attention_mask.to(device)\n",
    "    else:\n",
    "        attention_mask = None\n",
    "    \n",
    "    negative_prompt_embeds = text_encoder(\n",
    "        uncond_input.input_ids.to(device),\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "    prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "\n",
    "    return prompt_embeds\n",
    "\n",
    "def decode_latents(vae, latents):\n",
    "    latents = 1 / vae.config.scaling_factor * latents\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "    return image\n",
    "\n",
    "def prepare_mel(conditioning_mel_path, device):\n",
    "\n",
    "    conditioning_mel = np.load(conditioning_mel_path)\n",
    "\n",
    "    conditioning_mel_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        ]\n",
    "    )\n",
    "    conditioning_mel = conditioning_mel_transforms(conditioning_mel).repeat_interleave(3, dim=0)\n",
    "    c, h, w = conditioning_mel.shape\n",
    "    conditioning_mel = conditioning_mel.reshape(1, c, h, w)\n",
    "    conditioning_mel = conditioning_mel.to(device=device)\n",
    "    conditioning_mel = torch.cat([conditioning_mel] * 2)\n",
    "    return conditioning_mel\n",
    "\n",
    "def inference_mel_to_img(vae: AutoencoderKL, text_encoder: CLIPTextModel, tokenizer: CLIPTokenizer,\n",
    "                         unet: UNet2DConditionModel, controlnet: ControlNetModel, scheduler: PNDMScheduler,\n",
    "                        conditioning_mel_path, prompt, num_inference_steps=50, guidance_scale=7.5,\n",
    "                        device=torch.device(\"cuda:2\")):\n",
    "    \n",
    "    print(\"Device:\", device)\n",
    "    vae.to(device)\n",
    "    text_encoder.to(device)\n",
    "    unet.to(device)\n",
    "    controlnet.to(device)\n",
    "    vae.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    controlnet.requires_grad_(False)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        prompt_embeds = encode_prompt(tokenizer, text_encoder, prompt, device)\n",
    "        print(prompt_embeds.shape)\n",
    "        conditioning_mel = prepare_mel(conditioning_mel_path, device)\n",
    "\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = scheduler.timesteps\n",
    "\n",
    "        num_channels_latents = unet.in_channels\n",
    "        shape = (1, num_channels_latents, 512 // 8, 512 // 8)\n",
    "        latents = torch.randn(shape, device=device)\n",
    "        latents = latents.to(device)\n",
    "\n",
    "        # num_warmup_steps = len(timesteps) - num_inference_steps * scheduler.order\n",
    "        for t in tqdm(timesteps[:]):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            # controlnet(s) inference\n",
    "            down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                controlnet_cond=conditioning_mel,\n",
    "                conditioning_scale=1.0,\n",
    "                return_dict=False,\n",
    "            )\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                down_block_additional_residuals=down_block_res_samples,\n",
    "                mid_block_additional_residual=mid_block_res_sample,\n",
    "            ).sample\n",
    "\n",
    "            # perform guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        images = decode_latents(vae, latents)\n",
    "        images = (images * 255).round().astype(\"uint8\")\n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "        return pil_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\"/blob/v-yuancwang/DiffAudioImg/AudioControlNet_2class/checkpoint-8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = PNDMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "# unet.load_attn_procs(\"/blob/v-yuancwang/DiffAudioImg/AudioControlNet_lora/checkpoint-16000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning_mel_path = os.path.join(\"/blob/v-yuancwang/DiffAudioImg/VGGSound/data/vggsound/mel\",\n",
    "                                     \"-DSNfC2EJhU_20000_30000.npy\")\n",
    "\n",
    "inference_mel_to_img(vae, text_encoder, tokenizer,\n",
    "                    unet, controlnet, scheduler,\n",
    "                    conditioning_mel_path, prompt=\"\", num_inference_steps=100, guidance_scale=4.0,\n",
    "                    device=torch.device(\"cuda:2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(os.path.join(\"/blob/v-yuancwang/DiffAudioImg/VGGSound/data/vggsound/img_spilt\",\n",
    "                                \"2/-KrDQXnlXRw_10000_20000_2.jpg\"))\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
